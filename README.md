# ğŸ§  personal-data-pipeline

A modular, extensible data ingestion system designed to collect, store, and manage **personal data across multiple sources** â€” such as wearables, workouts, water intake, productivity tools, and more.

This project is focused on building a personal "data lakehouse" starting with the **bronze layer** (raw ingestion), using best practices from modern data engineering pipelines.

---

## ğŸ“ Project Goals

- âœ… Automate ingestion from various personal data sources (Garmin, Toggl, Water Tracker, Notion, etc.)
- âœ… Store all raw data in a structured, queryable format (PostgreSQL bronze tables)
- âœ… Maintain modular, source-isolated ETL pipelines
- âœ… Preserve raw historical data for future analytics or transformation
- âœ… Make ingestion resilient, testable, and scalable

---

## ğŸ—‚ Directory Structure

```
personal-data-pipeline/
â”‚
â”œâ”€â”€ dags/                      # Airflow DAGs (future orchestration)
â”‚
â”œâ”€â”€ pipelines/                 # One folder per data source
â”‚   â”œâ”€â”€ garmin/
â”‚   â”‚   â”œâ”€â”€ extract.py
â”‚   â”‚   â”œâ”€â”€ transform.py
â”‚   â”‚   â”œâ”€â”€ load.py
â”‚   â”‚   â””â”€â”€ schema.json
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ .env                   # Environment variables for local development
â”‚   â””â”€â”€ credentials_template.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ bronze/                # Optional raw file storage (JSON, CSV)
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ bronze/                # SQL table definitions for bronze layer
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ db.py                  # PostgreSQL connection management
â”‚   â”œâ”€â”€ logger.py              # Unified logging interface
â”‚   â””â”€â”€ api.py                 # Common API helpers (e.g., headers, retries)
â”‚
â”œâ”€â”€ tests/                     # Unit and integration tests
â”‚
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quickstart

### 1. Clone the Repo

```bash
git clone https://github.com/yourname/personal-data-pipeline.git
cd personal-data-pipeline
```

### 2. Create a Virtual Environment

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Configure Environment Variables

Create a `.env` file inside the `configs/` directory.

```env
PG_HOST=localhost
PG_PORT=5432
PG_DB=personal_data
PG_USER=postgres
PG_PASS=password

GARMIN_CLIENT_ID=your_id
GARMIN_CLIENT_SECRET=your_secret
...
```

### 4. Run Ingestion for a Source

```bash
python main.py garmin
```

---

## ğŸ” Supported Data Sources

| Source       | Status     | Notes |
|--------------|------------|-------|
| Garmin       | âœ… Planned | Sleep, HR, steps, stress |
| Toggl        | âœ… Planned | Time tracking |
| WaterBottle  | ğŸ”œ          | Daily water intake via API |
| Notion       | ğŸ”œ          | Manual tracking tables (e.g. mood, weight) |
| Airtable     | ğŸ”œ          | Possession inventory, body proportions |
| Weather      | ğŸ”œ          | External enrichments based on location/date |

---

## ğŸ›  ETL Design Pattern

Each source implements a simple ETL contract:

```python
# extract.py
def fetch_data():
    return raw_json_data

# transform.py
def clean(data):
    return transformed_data

# load.py
def load_to_bronze(data):
    insert_into_postgres(data)
```

Each module writes data to a **bronze layer table** in PostgreSQL, with the following common schema:

```sql
CREATE TABLE bronze_<source> (
    id SERIAL PRIMARY KEY,
    source TEXT,
    raw_data JSONB,
    ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_timestamp TIMESTAMP,
    raw_id TEXT
);
```

---

## ğŸ§ª Testing

Tests live in the `tests/` folder and include:

- Unit tests for transforms
- Mocked ingestion runs
- Database insertion checks

Run tests with:

```bash
pytest tests/
```

---

## ğŸ— Future Roadmap

| Feature                        | Priority |
|-------------------------------|----------|
| Airflow DAGs for orchestration| High     |
| dbt integration for modeling  | Medium   |
| Grafana dashboards            | Medium   |
| Webhooks where available      | Medium   |
| Cloud storage (S3) + Athena   | Medium   |
| Machine learning models       | Low      |

---

## ğŸ” Security Notes

- API credentials are stored locally in `.env` during development.
- For production, secrets should be stored in a manager like **AWS Secrets Manager**, **Vault**, or **environment injection** via CI/CD.
- Do not commit `.env` or credentials to Git.

---

## ğŸ¤ Contributions

This is a personal project and not yet open for community contributions. However, if you're building something similar and want to collaborate, feel free to reach out.

---

## ğŸ“¬ Contact

Created by **[Your Name]**  
Twitter: [@yourhandle]  
Email: your@email.com

---

## ğŸ§  Philosophy

> â€œKnow thyself.â€ â€” Socrates  
>
> This project is an attempt to truly understand personal patterns through data â€” by logging, visualizing, and learning from every aspect of life.
